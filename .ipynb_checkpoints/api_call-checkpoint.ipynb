{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time \n",
    "import random\n",
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I created a .py file that has both the api id and api key. Here, imported them as the variables app_id and app_key\n",
    "from keys import app_id, app_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using beautiful soup, I scraped the state abbreviations and created a DataFrame. \n",
    "\n",
    "url='https://www23.statcan.gc.ca/imdb/p3VD.pl?Function=getVD&TVD=53971'\n",
    "page = requests.get(url)\n",
    "bs = BS(page.content, 'html.parser')\n",
    "states = pd.DataFrame([[i.text for i in x.findAll('td')] for x in bs.table.findAll('tr')][1:])[2].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making API calls with Charity Navigator API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL 2\n",
      "AK 2\n",
      "AZ 2\n",
      "AR 2\n",
      "CA 3\n",
      "CO 2\n",
      "CT 2\n",
      "DE 2\n",
      "DC 2\n",
      "FL 2\n",
      "GA 2\n",
      "HI 2\n",
      "ID 2\n",
      "IL 2\n",
      "IN 2\n",
      "IA 2\n",
      "KS 2\n",
      "KY 2\n",
      "LA 2\n",
      "ME 2\n",
      "MD 2\n",
      "MA 2\n",
      "MI 2\n",
      "MN 2\n",
      "MS 2\n",
      "MO 2\n",
      "MT 2\n",
      "NE 2\n",
      "NV 2\n",
      "NH 2\n",
      "NJ 2\n",
      "NM 2\n",
      "NY 2\n",
      "NC 2\n",
      "ND 2\n",
      "OH 2\n",
      "OK 2\n",
      "OR 2\n",
      "PA 2\n",
      "RI 2\n",
      "SC 2\n",
      "SD 2\n",
      "TN 2\n",
      "TX 2\n",
      "UT 2\n",
      "VT 2\n",
      "VA 2\n",
      "WA 2\n",
      "WV 2\n",
      "WI 2\n",
      "WY 2\n"
     ]
    }
   ],
   "source": [
    "#Instantiated two empty list to populate with the api calls\n",
    "datarating, databadlist = [] , []\n",
    "\n",
    "#Created a for loop using the states abbreviations and nested another for loop within in it to iterate through pages. If the status code is successful (status_code = 200) then it will\n",
    "#append the the another empty list I instantiated within the nested loop called lst. It will grab the data I have designated and if the data point is complete it will append the datarating\n",
    "#list above, if the organization is missing any of the designated information it will then append the databadlist. If the response isn't 200, the code will print the state and the page\n",
    "#where it broke.\n",
    "for state in states:\n",
    "    for page in range(1,100):\n",
    "        url = f'https://api.data.charitynavigator.org/v2/Organizations?app_id={app_id}&app_key={app_key}&rated=true&state={state}&pageSize=1000&pageNum={page}'\n",
    "        response = requests.get(url)        \n",
    "        if response.status_code == 200:\n",
    "            polls = response.json()\n",
    "            for x in polls:\n",
    "                try:\n",
    "                    lst = []\n",
    "                    lst.append(x['mission'])\n",
    "                    lst.append(x['tagLine'])\n",
    "                    lst.append(x['charityName'])\n",
    "                    lst.append(x['category']['categoryName'])\n",
    "                    lst.append(x['category']['categoryID'])\n",
    "                    lst.append(x['cause']['causeName'])\n",
    "                    lst.append(x['cause']['causeID'])\n",
    "                    lst.append(x['mailingAddress']['city'])\n",
    "                    lst.append(x['mailingAddress']['stateOrProvince'])\n",
    "                    lst.append(x['mailingAddress']['postalCode'])\n",
    "                    lst.append(x['currentRating']['score'])\n",
    "                    lst.append(x['currentRating']['rating'])\n",
    "                    lst.append(x['advisories']['severity'])\n",
    "                    current_accountability_rating = x['currentRating']['accountabilityRating']\n",
    "                    lst.append(current_accountability_rating['score'])\n",
    "                    lst.append(current_accountability_rating['rating'])\n",
    "                    lst.append(x['irsClassification']['nteeType'])\n",
    "                    lst.append(x['irsClassification']['classification'])\n",
    "                    lst.append(x['irsClassification']['affiliation'])\n",
    "                    lst.append(x['irsClassification']['foundationStatus'])\n",
    "                    lst.append(x['irsClassification']['nteeClassification'])\n",
    "                    lst.append(x['irsClassification']['deductibility'])\n",
    "                    lst.append(x['irsClassification']['subsection'])\n",
    "                    lst.append(x['irsClassification']['assetAmount'])\n",
    "                    lst.append(x['irsClassification']['incomeAmount'])\n",
    "                    current_financial_rating = x['currentRating']['financialRating']\n",
    "                    lst.append(current_financial_rating['score'])\n",
    "                    lst.append(current_financial_rating['rating'])\n",
    "                    datarating.append(lst)\n",
    "                except:\n",
    "                    databadlist.append(x)\n",
    "        else:\n",
    "            print(state, page)\n",
    "            break\n",
    "\n",
    "# The timer is used to make sure it pulls the information in time intervals to make it appear as 'human' as possible.        \n",
    "        time.sleep(random.choice([x/10 for x in range(7,13)]))\n",
    "    \n",
    "# This turns the datarating list into a data frame and creates a .csv file that I can import into another notebook  \n",
    "    df = pd.DataFrame(datarating)\n",
    "#    df.to_csv('datarating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# The above code returned 9005 data points, I wanted to see if I can pull additional data so I used a similar code but did not iterate through the states to get different data points\n",
    "datarating2, databadlist2 = [] , []\n",
    "\n",
    "for page in range(1,100):\n",
    "    url = f'https://api.data.charitynavigator.org/v2/Organizations?app_id={app_id}&app_key={app_key}&rated=true&pageSize=1000&pageNum={page}'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        polls = response.json()\n",
    "        for x in polls:\n",
    "            try:\n",
    "                lst = []\n",
    "                lst.append(x['mission'])\n",
    "                lst.append(x['tagLine'])\n",
    "                lst.append(x['charityName'])\n",
    "                lst.append(x['category']['categoryName'])\n",
    "                lst.append(x['category']['categoryID'])\n",
    "                lst.append(x['cause']['causeName'])\n",
    "                lst.append(x['cause']['causeID'])\n",
    "                lst.append(x['mailingAddress']['city'])\n",
    "                lst.append(x['mailingAddress']['stateOrProvince'])\n",
    "                lst.append(x['mailingAddress']['postalCode'])\n",
    "                lst.append(x['currentRating']['score'])\n",
    "                lst.append(x['currentRating']['rating'])\n",
    "                lst.append(x['advisories']['severity'])\n",
    "                current_accountability_rating = x['currentRating']['accountabilityRating']\n",
    "                lst.append(current_accountability_rating['score'])\n",
    "                lst.append(current_accountability_rating['rating'])\n",
    "                lst.append(x['irsClassification']['nteeType'])\n",
    "                lst.append(x['irsClassification']['classification'])\n",
    "                lst.append(x['irsClassification']['affiliation'])\n",
    "                lst.append(x['irsClassification']['foundationStatus'])\n",
    "                lst.append(x['irsClassification']['nteeClassification'])\n",
    "                lst.append(x['irsClassification']['deductibility'])\n",
    "                lst.append(x['irsClassification']['subsection'])\n",
    "                lst.append(x['irsClassification']['assetAmount'])\n",
    "                lst.append(x['irsClassification']['incomeAmount'])\n",
    "                current_financial_rating = x['currentRating']['financialRating']\n",
    "                lst.append(current_financial_rating['score'])\n",
    "                lst.append(current_financial_rating['rating'])\n",
    "                datarating2.append(lst)\n",
    "            except:\n",
    "                databadlist2.append(x)\n",
    "    else:\n",
    "        print(page)\n",
    "        break\n",
    "    time.sleep(random.choice([x/10 for x in range(7,13)]))\n",
    "df2 = pd.DataFrame(datarating2)\n",
    "#df2.to_csv('datarating2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the above code I was able to pull an additional 8958 data points for a total of 17963 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Final DataFrame and .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatinating both of the dataframes into one, called df_concat\n",
    "df_concat = pd.concat([df, df2])\n",
    "\n",
    "#I want to make sure that all of the data points are unique, I drop the duplicates and end up with 10851 rows\n",
    "df_concat = df_concat.drop_duplicates().reset_index()\n",
    "\n",
    "#I then take that dataframe and turn it into a .csv file\n",
    "df_concat.to_csv('final_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
